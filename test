docker build -t simple_pipeline_spark:latest -f infra/dockerfiles/spark.dockerfile .

docker \
run \
-v $PWD/jobs/admission.py:/app/main.py \
-v $PWD/jobs/data_platform:/app/data_platform \
simple_pipeline_spark:latest \
--entrypoint /opt/spark/bin/spark-submit --master local main.py "{\"incremental_strategy\":{\"type\":\"datetime\",\"column_name\":\"date\"},\"cleansing\":{\"keep_most_recent_register_based_on_column\":{\"column_name\":\"date\",\"identity_column_name\":\"id\"}},\"input\":{\"type\":\"json\",\"path\":\"file://///transient/upstream_default\"},\"output\":{\"type\":\"parquet\",\"path\":\"file://///raw/upstream_default\",\"partition_by\":[\"year\",\"month\",\"day\"]}}"




docker \
run -it \
-v $PWD/jobs/admission.py:/app/main.py \
-v $PWD/jobs/data_platform:/app/data_platform \
simple_pipeline_spark:latest \
--entrypoint /opt/spark/bin/pyspark